# -*- coding: utf-8 -*-
"""Completed-PIMA-Diabetes-Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1li512FbDrFQdsplm8ZZK4CwycN1iAwtH
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
sns.set(color_codes=True)
# %matplotlib inline
import warnings
warnings.filterwarnings('ignore')

#Importing dataset
df = pd.read_csv('https://raw.githubusercontent.com/shikharsingh2697/Diabetes-Prediction/master/diabetes.csv')

df.shape

df.head(5)

df.tail(5)

# Statistical summary
df.describe().T

sns.countplot(df['Outcome'],label="Count")

n1 = len(df)
n2 = len(df.loc[df['Outcome'] == 1])
n3 = len(df.loc[df['Outcome'] == 0])
print("True cases:  {0} ({1:2.2f}%)".format(n2, ((1.00 * n2)/(1.0 * n1)) * 100))
print("False cases: {0} ({1:2.2f}%)".format(n3, (( 1.0 * n3)/(1.0 * n1)) * 100))

# Count of #NaN #not a number
df.isnull().sum()

# Heatmap to check for correlation
plt.figure(figsize=(10,10))
sns.heatmap(df.corr(), cmap="YlGnBu", annot= True,)
plt.title('Correlation between features');
plt.show()

sns.set(style="whitegrid")
labels = ['Healthy', 'Diabetic']
sizes = df['Outcome'].value_counts(sort = True)

colors = ["lightblue","red"]
explode = (0.05,0) 
 
plt.figure(figsize=(7,7))
plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=90,)

plt.title('Number of diabetes in the dataset')
plt.show()

"""üìå From above pie chart, we can say that around 65% of the people are Healthy and 35% are Diabetic

# Distribution Plot 
 Healthy vs Diabetic by Pregnancy
"""

plt.style.use("classic")
plt.figure(figsize=(10,10))

sns.distplot(df[df['Outcome'] == 0]["Pregnancies"], color='green') # Healthy - green
sns.distplot(df[df['Outcome'] == 1]["Pregnancies"], color='red') # Diabetic - Red

plt.title('Healthy vs Diabetic by Pregnancy', fontsize=15)
plt.xlim([-5,20])
plt.grid(linewidth = 0.7)
plt.show()

"""üìå From above graph, we can say that the Pregnancy isn't likely cause for diabetes as the distribution between the Healthy and Diabetic is almost same.

Healthy vs Diabetic by Glucose
"""

plt.style.use("classic")
plt.figure(figsize=(10,10))

sns.distplot(df[df['Outcome'] == 0]["Glucose"], color='green') # Healthy - green
sns.distplot(df[df['Outcome'] == 1]["Glucose"], color='red') # Diabetic - Red

plt.title('Healthy vs Diabetic by Glucose', fontsize=15)
plt.xlim([-5,250])
plt.grid(linewidth = 0.7)
plt.show()

"""üìå Diabetes is a disease that occurs when your blood glucose, also called blood sugar, is too high. Blood glucose is your main source of energy and comes from the food you eat.

üìå The Glucose level for a Normal Adult is around 120-130mg/dl anything above it means that the person is likely suffering from pre-diabetes and diabetes.

üìå From above graph, we can see the the Healthy person are more around 120mg/dl but it then gradually drops, and for diabetic person it is vice versa.

Healthy vs Diabetic by Blood Pressure
"""

plt.style.use("classic")
plt.figure(figsize=(10,10))

sns.distplot(df[df['Outcome'] == 0]["BloodPressure"], color='green') # Healthy - green
sns.distplot(df[df['Outcome'] == 1]["BloodPressure"], color='red') # Diabetic - Red

plt.title('Healthy vs Diabetic by Blood Pressure', fontsize=15)
plt.xlim([-5,150])
plt.grid(linewidth = 0.7)
plt.show()

"""üìå High blood pressure (also known as ‚Äúhypertension‚Äù) is very common in people with diabetes. In fact, the two conditions often go hand-in-hand because they can both result from the same lifestyle factors.

üìå Diabetes damages arteries and makes them targets for hardening, called atherosclerosis. That can cause high blood pressure, which if not treated, can lead to trouble including blood vessel damage, heart attack, and kidney failure.

üìå For a Normal person BP should be at or below 120/80 mm Hg, the person with hypertension can be above 139/89 mm Hg.

üìå From above graph, we can say that, diabetic and healthy people are evenly distributed with low and normal BP but, there are less healthy people who have high BP.

Healthy vs Diabetic by Skin Thickness
"""

plt.style.use("classic")
plt.figure(figsize=(10,10))

sns.distplot(df[df['Outcome'] == 0]["SkinThickness"], color='green') # Healthy - green
sns.distplot(df[df['Outcome'] == 1]["SkinThickness"], color='red') # Diabetic - Red

plt.title('Healthy vs Diabetic by Skin Thickness', fontsize=15)
plt.xlim([-5,120])
plt.grid(linewidth = 0.7)
plt.show()

"""üìå Changes to the blood vessels because of diabetes can cause a skin condition called diabetic dermopathy. Dermopathy appears as scaly patches that are light brown or red, often on the front of the legs. The patches do not hurt, blister, or itch, and treatment generally is not necessary.

üìå From above graph, the distribution between healthy and diabetic people are around same for skin thickness.

Healthy vs Diabetic by Insulin
"""

plt.style.use("classic")
plt.figure(figsize=(10,10))

sns.distplot(df[df['Outcome'] == 0]["Insulin"], color='green') # Healthy - green
sns.distplot(df[df['Outcome'] == 1]["Insulin"], color='red') # Diabetic - Red

plt.title('Healthy vs Diabetic by Insulin', fontsize=15)
plt.xlim([-10,900])
plt.grid(linewidth = 0.7)
plt.show()

"""üìå Insulin is a hormone that your pancreas makes to allow cells to use glucose. When your body isn't making or using insulin correctly, you can take man-made insulin to help control your blood sugar. Many types can be used to treat diabetes.

üìå Insulin helps control blood glucose levels by signaling the liver and muscle and fat cells to take in glucose from the blood. Insulin therefore helps cells to take in glucose to be used for energy. If the body has sufficient energy, insulin signals the liver to take up glucose and store it as glycogen.

üìå From above graph, we can see that there are diabetic people increase as the levels of insulin gradually increases. There are more healthy people around insulin levels 0-100.

Healthy vs Diabetic by BMI
"""

plt.style.use("classic")
plt.figure(figsize=(10,10))

sns.distplot(df[df['Outcome'] == 0]["BMI"], color='green') # Healthy - green
sns.distplot(df[df['Outcome'] == 1]["BMI"], color='red') # Diabetic - Red

plt.title('Healthy vs Diabetic by BMI', fontsize=15)
plt.xlim([-10,75])
plt.grid(linewidth = 0.7)
plt.show()

"""üìå Being overweight (BMI of 25-29.9), or affected by obesity (BMI of 30-39.9) or morbid obesity (BMI of 40 or greater), greatly increases your risk of developing type 2 diabetes. The more excess weight you have, the more resistant your muscle and tissue cells become to your own insulin hormone.

üìå From above graph we can determine that, as the BMI increases the person likely being healthy decreases and being diabetic increases.

Healthy vs Diabetic by Diabetes Pedigree Function
"""

plt.style.use("classic")
plt.figure(figsize=(10,10))

sns.distplot(df[df['Outcome'] == 0]["DiabetesPedigreeFunction"], color='green') # Healthy - green
sns.distplot(df[df['Outcome'] == 1]["DiabetesPedigreeFunction"], color='red') # Diabetic - Red

plt.title('Healthy vs Diabetic by Diabetes Pedigree Function', fontsize=15)
plt.xlim([-1,3])
plt.grid(linewidth = 0.7)
plt.show()

"""üìå Diabetes Pedigree Function is a function which scores likelihood of diabetes based on family history. It provided some data on diabetes mellitus history in relatives and the genetic relationship of those relatives to the patient.

üìå From above graph, as thefunction increase the diabetic people increases, showing that the diabetes could be hereditary for that individual.

Healthy vs Diabetic by Age
"""

plt.style.use("classic")
plt.figure(figsize=(10,10))

sns.distplot(df[df['Outcome'] == 0]["Age"], color='green') # Healthy - green
sns.distplot(df[df['Outcome'] == 1]["Age"], color='red') # Diabetic - Red

plt.title('Healthy vs Diabetic by Age', fontsize=15)
plt.xlim([0,100])
plt.grid(linewidth = 0.7)
plt.show()

"""üìå As the person ages, they are at high risk for the development of type 2 diabetes due to the combined effects of increasing insulin resistance and impaired pancreatic islet function with aging.

üìå From above graph, we can see that there are more healthy people around 20-25 age but as the age gradually increases so does the people being diabetic, this shows that age and diabetes go hand in hand.
"""

sns.pairplot(data=df,hue='Outcome',diag_kind='kde')
plt.show()

"""Dataset spliting"""

x = df.iloc[:, :-1].values
y = df.iloc[:, -1].values

"""Divided into train-test"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state= 0)

print("Number transactions x_train dataset: ", x_train.shape)
print("Number transactions y_train dataset: ", y_train.shape)
print("Number transactions x_test dataset: ", x_test.shape)
print("Number transactions y_test dataset: ", y_test.shape)

from sklearn.preprocessing import StandardScaler 
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, classification_report, roc_curve, plot_roc_curve, auc, precision_recall_curve, plot_precision_recall_curve, average_precision_score
from sklearn.model_selection import cross_val_score

models = []
models.append(['Logistic Regreesion', LogisticRegression(random_state=0)])
models.append(['GaussianNB', GaussianNB()])
models.append(['Random Forest', RandomForestClassifier(random_state=0)])


lst_1= []

for m in range(len(models)):
    lst_2= []
    model = models[m][1]
    model.fit(x_train, y_train)
    y_pred = model.predict(x_test)
    cm = confusion_matrix(y_test, y_pred)  #Confusion Matrix
    accuracies = cross_val_score(estimator = model, X = x_train, y = y_train, cv = 10)   #K-Fold Validation
    roc = roc_auc_score(y_test, y_pred)  #ROC AUC Score
    precision = precision_score(y_test, y_pred)  #Precision Score
    recall = recall_score(y_test, y_pred)  #Recall Score
    f1 = f1_score(y_test, y_pred)  #F1 Score
    print(models[m][0],':')
    print(cm)
    print('Accuracy Score: ',accuracy_score(y_test, y_pred))
    print('')
    print("K-Fold Validation Mean Accuracy: {:.2f} %".format(accuracies.mean()*100))
    print('')
    print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))
    print('')
    print('ROC AUC Score: {:.2f}'.format(roc))
    print('')
    print('Precision: {:.2f}'.format(precision))
    print('')
    print('Recall: {:.2f}'.format(recall))
    print('')
    print('F1: {:.2f}'.format(f1))
    print('-----------------------------------')
    print('')
    lst_2.append(models[m][0])
    lst_2.append((accuracy_score(y_test, y_pred))*100) 
    lst_2.append(accuracies.mean()*100)
    lst_2.append(accuracies.std()*100)
    lst_2.append(roc)
    lst_2.append(precision)
    lst_2.append(recall)
    lst_2.append(f1)
    lst_1.append(lst_2)

df = pd.DataFrame(lst_1, columns= ['Model', 'Accuracy', 'K-Fold Mean Accuracy', 'Std. Deviation', 'ROC AUC', 'Precision', 'Recall', 'F1'])

df.sort_values(by= ['Accuracy', 'K-Fold Mean Accuracy'], inplace= True, ascending= False)

df

from sklearn.model_selection import GridSearchCV

"""üìå The GridSearchCV is a library function that is a member of sklearn's model_selection package. It helps to loop through predefined hyperparameters and fit your estimator (model) on your training set. So, in the end, you can select the best parameters from the listed hyperparameters."""

grid_models = [(LogisticRegression(),[{'C':[0.25,0.5,0.75,1],'random_state':[0]}]), 
               (GaussianNB(),[{'var_smoothing': [1e-09]}]), 
                (RandomForestClassifier(),[{'n_estimators':[100,150,200],'criterion':['gini','entropy'],'random_state':[0]}])
            ]

for i,j in grid_models:
    grid = GridSearchCV(estimator=i,param_grid = j, scoring = 'accuracy',cv = 10)
    grid.fit(x_train, y_train)
    best_accuracy = grid.best_score_
    best_param = grid.best_params_
    print('{}:\nBest Accuracy : {:.2f}%'.format(i,best_accuracy*100))
    print('Best Parameters : ',best_param)
    print('')
    print('----------------')
    print('')

"""üìå After Grid Search, we got best parameters for all the models. Now, we going to tune hyperparameters see how to it perform.

üìå True Positives (TP) - These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes.

üìå True Negatives (TN) - These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no.

üìå False Positives (FP) ‚Äì When actual class is no and predicted class is yes.

üìå False Negatives (FN) ‚Äì When actual class is yes but predicted class in no.

üìå Accuracy - Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations.
                 Accuracy = TP+TN/TP+FP+FN+TN

üìå Precision - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.
                 Precision = TP/TP+FP

üìå Recall (Sensitivity) - Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes.
                 Recall = TP/TP+FN

üìå F1 score - F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account.
                 F1 Score = 2(Recall Precision) / (Recall + Precision)

üìå Support - Support is the number of actual occurrences of the class in the specified dataset. Support doesn‚Äôt change between models but instead diagnoses the evaluation process.

Logistic regression
"""

#Fitting Logistic Regression Model
classifier = LogisticRegression(C= 1, random_state= 0)
classifier.fit(x_train, y_train)
y_pred = classifier.predict(x_test)
y_prob = classifier.predict_proba(x_test)[:,1]
cm = confusion_matrix(y_test, y_pred)

print(classification_report(y_test, y_pred))
print(f'ROC AUC score: {roc_auc_score(y_test, y_prob)}')
print('Accuracy Score: ',accuracy_score(y_test, y_pred))

# Visualizing Confusion Matrix
plt.figure(figsize = (6, 6))
sns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, 
            yticklabels = ['Healthy', 'Diabetic'], xticklabels = ['Predicted Healthy', 'Predicted Diabetic'])
plt.yticks(rotation = 0)
plt.show()

# Roc AUC Curve
false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(false_positive_rate, true_positive_rate)

sns.set_theme(style = 'white')
plt.figure(figsize = (6, 6))
plt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')
plt.axis('tight')
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.title('ROC AUC Curve')
plt.legend()
plt.show()

#Precision Recall Curve
plt.figure(figsize = (6, 6))
average_precision = average_precision_score(y_test, y_prob)
disp = plot_precision_recall_curve(classifier, x_test, y_test)
plt.title('Precision-Recall Curve')
plt.show()

"""Naive Bayes"""

#Fitting GaussianNB Model
classifier = GaussianNB(var_smoothing= 1e-09)
classifier.fit(x_train, y_train)
y_pred = classifier.predict(x_test)
y_prob = classifier.predict_proba(x_test)[:,1]
cm = confusion_matrix(y_test, y_pred)

print(classification_report(y_test, y_pred))
print(f'ROC AUC score: {roc_auc_score(y_test, y_prob)}')
print('Accuracy Score: ',accuracy_score(y_test, y_pred))

# Visualizing Confusion Matrix
plt.figure(figsize = (6, 6))
sns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, 
            yticklabels = ['Healthy', 'Diabetic'], xticklabels = ['Predicted Healthy', 'Predicted Diabetic'])
plt.yticks(rotation = 0)
plt.show()

# Roc AUC Curve
false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(false_positive_rate, true_positive_rate)

sns.set_theme(style = 'white')
plt.figure(figsize = (6, 6))
plt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')
plt.axis('tight')
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.title('ROC AUC Curve')
plt.legend()
plt.show()

#Precision Recall Curve
average_precision = average_precision_score(y_test, y_prob)
disp = plot_precision_recall_curve(classifier, x_test, y_test)
plt.title('Precision-Recall Curve')
plt.show()

"""Random forest classifier"""

#Fitting RandomForestClassifier Model
classifier = RandomForestClassifier(criterion= 'entropy', n_estimators= 200, random_state= 0)
classifier.fit(x_train, y_train)
y_pred = classifier.predict(x_test)
y_prob = classifier.predict_proba(x_test)[:,1]
cm = confusion_matrix(y_test, y_pred)

print(classification_report(y_test, y_pred))
print(f'ROC AUC score: {roc_auc_score(y_test, y_prob)}')
print('Accuracy Score: ',accuracy_score(y_test, y_pred))

# Visualizing Confusion Matrix
plt.figure(figsize = (6, 6))
sns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, 
            yticklabels = ['Healthy', 'Diabetic'], xticklabels = ['Predicted Healthy', 'Predicted Diabetic'])
plt.yticks(rotation = 0)
plt.show()

# Roc AUC Curve
false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(false_positive_rate, true_positive_rate)

sns.set_theme(style = 'white')
plt.figure(figsize = (6, 6))
plt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')
plt.axis('tight')
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.title('ROC AUC Curve')
plt.legend()
plt.show()

#Precision Recall Curve
average_precision = average_precision_score(y_test, y_prob)
disp = plot_precision_recall_curve(classifier, x_test, y_test)
plt.title('Precision-Recall Curve')
plt.show()

